Question 10. Pour un reduce 10 000 000 fois

Mult double classique avec fno tree vectorize:
CPI: 38.9547
IPC: 0.0256708

Mult double classique avec les options SIMD:
CPI: 0.214308
IPC: 4.66618

Mult double SIMD avec les options SIMD:
CPI: 0.0762697
IPC: 13.1114

Add int32 classique avec fno tree vectorize:
CPI: 315.638
IPC: 0.00316819

Add int32 classique avec les options SIMD:
CPI: 0.966603
IPC: 1.03455

Add int32 SIMD avec les options SIMD:
CPI: 0.735934
IPC: 1.35882

////////////////:

Question 11 avec des vecteur float de taille 1 000 000

min float classique avec fno tree vectorize:
CPI: 0.255545
IPC: 3.9132

min float classique avec les options SIMD:
CPI: 0.0193462
IPC: 51.6896

min SIMD avec les options SIMD:
CPI: 0.018452
IPC: 54.1947

min avec std::min_element + options SIMD:
CPI: 0.0195798
IPC: 51.073


/////////////

Question 12 

Meilleur algo quand j est le plus profond

Voir code pour les fonctions et le matrixDouble SIMD


////////////

Question 13 
/*
13.1. On a une matrice (i,j) de taille m lignes *n colonnes avec i = ligne et j = colonnes
    et on a notre vecteur(x) unidimentionnel de taille n*m.
    Pour retrouver un élément a l'indice i,j, il faut :
    - parcourir n*i (n fois le nombre d'élément d'une ligne)
    - parcourir j éléménet pour accéder à la colonne souhaité 

    Ce qui nous donne M[i,j] = V[i*n + j];

13.2. test avec deux matrices (1,2,3,4) de taille n*n (n = 2)
 mat_carré 0 : 1
 mat_carré 1 : 2
 mat_carré 2 : 3
 mat_carré 3 : 4

 mat_res 0 : 7
 mat_res 1 : 10
 mat_res 2 : 15
 mat_res 3 : 22 // Valide

// Pour n = 1000 sans optimisation () et N = 2*(n*n*n)
1:
CPI: 14.4197
IPC: 0.0693494

2:
CPI: 13.8366
IPC: 0.072272

3:
CPI: 19.6217
IPC: 0.0509639

4:
CPI: 20.338
IPC: 0.049169

5:
CPI: 17.5249
IPC: 0.0570615

6:
CPI: 17.8895
IPC: 0.0558987


Le meilleur semble être le 2 (Ordre i k j) avec le 1 qui s'en rapproche (Ordre k i j)
Ce qui semble cohérent car J c'est le plus profond dans le parcours des matrices;

13.3.
MultMat version SIMD et FMA sans l'option SIMD:
CPI: 10.4749
IPC: 0.0954664

Donc bien plus performant que le reste au dessus.

13.4. avec le SIMD option activé , on constate que tout ce vaut plus ou moins:
    Donc cela signifie que le compilateur parallelise naturellement.


/////////////////////

Question 14

14.1 Transpose naïf : 

 vec initial : 
8 , 2 , 4 , 2
6 , 10 , 8 , 4
11 , 2 , 12 , 9
1 , 0 , 6 , 8
 vec transposed : 
8 , 6 , 11 , 1
2 , 10 , 2 , 0
4 , 8 , 12 , 6
2 , 4 , 9 , 8

ça fonctionne

14.2

 vec transposed : 
8 , 6 , 11 , 1
2 , 10 , 2 , 0
4 , 8 , 12 , 6
2 , 4 , 9 , 8
 vec SIMD transposed : 
8 , 6 , 11 , 1
2 , 10 , 2 , 0
4 , 8 , 12 , 6
2 , 4 , 9 , 8


14.3

 initial : 
13 , 1 , 12 , 10 , 8 , 10 , 1 , 12
9 , 1 , 2 , 7 , 5 , 4 , 8 , 1
0 , 6 , 7 , 1 , 11 , 8 , 12 , 9
2 , 5 , 2 , 13 , 7 , 10 , 14 , 12
12 , 3 , 14 , 12 , 13 , 1 , 1 , 7
9 , 3 , 6 , 14 , 14 , 7 , 8 , 14
5 , 0 , 8 , 1 , 1 , 5 , 11 , 3
2 , 5 , 1 , 1 , 0 , 0 , 14 , 12
 transposed standard : 
13 , 9 , 0 , 2 , 12 , 9 , 5 , 2
1 , 1 , 6 , 5 , 3 , 3 , 0 , 5
12 , 2 , 7 , 2 , 14 , 6 , 8 , 1
10 , 7 , 1 , 13 , 12 , 14 , 1 , 1
8 , 5 , 11 , 7 , 13 , 14 , 1 , 0
10 , 4 , 8 , 10 , 1 , 7 , 5 , 0
1 , 8 , 12 , 14 , 1 , 8 , 11 , 14
12 , 1 , 9 , 12 , 7 , 14 , 3 , 12
 transposed SIMD : 
13 , 9 , 0 , 2 , 12 , 9 , 5 , 2
1 , 1 , 6 , 5 , 3 , 3 , 0 , 5
12 , 2 , 7 , 2 , 14 , 6 , 8 , 1
10 , 7 , 1 , 13 , 12 , 14 , 1 , 1
8 , 5 , 11 , 7 , 13 , 14 , 1 , 0
10 , 4 , 8 , 10 , 1 , 7 , 5 , 0
1 , 8 , 12 , 14 , 1 , 8 , 11 , 14
12 , 1 , 9 , 12 , 7 , 14 , 3 , 12

ça a l'air de fonctionner 
Version naive avec N = n*n (fno tree vectorize): 
CPI: 116.252
IPC: 0.008602

Version SIMD avec  N = n*n :
CPI: 32.1858
IPC: 0.0310696

4* plus interessant d'avoir le SIMD.


///////////////

/Question 15


Pour les test, j'ai décidé de faire des fonctions avec Template

POur ma fonction On2 j'ai choisi le tri par selection avec comme N = (n * n-1) / 2

Pour ma fonction Onlogn j'ai choisi le tri fusion/dichotomique avec comme N = n * log (n)

On2:

2^17 : nb secondes: 52.6463
2^18 : ne repond pas après + 1 min

Pas eu le temps d'implémenter le tru fustion :



A partir de 2^12, des secondes se font remarquer avant d'avoir le resultat.

Pour 2^